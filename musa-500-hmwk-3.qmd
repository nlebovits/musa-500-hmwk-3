---
title: "MUSA 500, Homework #3"
author: "Minwook Kang, Ann Zhang, and Nissim Lebovits"
date: today
title-block-banner: "#18bc9c"
title-block-banner-color: "white"
format: 
  html:
    toc: true
    theme: flatly
    code-fold: true
    code-summary: "Show the code"
editor: visual
execute:
  warning: false
  error: false
  messages: false
project:
  type: website
  output-dir: docs
---

```{r setup}
library(tidyverse)
library(sf)
library(janitor)
#library(ggthemr) # mincomment : its not working on my pc
library(ggpubr)
library(ggrepel)
library(purrr)
#install.packages("kableExtra")
library(kableExtra)
library(caret)
library(corrplot)
# install.packages("prediction")
library(prediction)

#ggthemr("pale") #set global ggplot theme
options(scipen = 999) # turn off scientific notation

library(aod)
library(rms)
library(gmodels)

library(ROCR)

library(crosstable)
library(flextable)

#knitr::opts_knit$set(root.dir = "C:/Users/Nissim/Desktop/Fall 2022/Spat Stats/Homeworks/musa-500-hmwk-3")

#import data
crash_data = read.csv("https://github.com/kmu973/tempDataRepo/raw/main/Logistic%20Regression%20Data.csv") |>
              clean_names()


```

## Introduction

### The Problem

The goal of the current assignment is to identify predictors of accidents related to drunk driving. The data used in this assignment come from a data set containing all 53,260 car crashes in the City of Philadelphia for the years 2008 -- 2012. The data set was compiled by the Pennsylvania Department of Transportation, and is made available to the public at OpenDataPhilly.org. In the past, Azavea, one of Philadelphia's most prominent GIS software development firms, has used these data for a number of interesting analyses, which have been published on the company's website.

Because the crash data are geocoded, it is possible to spatially join the data to the 2000 Census block group level data set that was used for the two previous homework assignments. After the spatial join, each crash point contains the median household income and the percent of individuals with at least a bachelor's degree in the block group where the crash took place.

Even though the original data set has a total of 53,260 car crashes, for the sake of this assignment, we remove the 9,896 crash locations which took place in non-residential block groups, where median household income and vacancy rates are 0, from the data set. The final data set contains the 43,364 crashes that took place in Philadelphia's residential block groups. Here, we will be regressing the binary dependent variable, DRINKING_D, on the following binary and continuous predictors: FATAL_OR_M, OVERTURNED, CELL_PHONE, SPEEDING, AGGRESSIVE, DRIVER1617, DRIVER65PLUS, PCTBACHMOR, and MEDHHINC.

### Tools

## Methods

### Issues with OLS Regression

Let's begin by recalling the five assumptions of OLS regression:

1.  A linear relationship between the dependent variable and predictors
2.  Normality of residuals
3.  Homoscedasticity
4.  Independence of observations (no spatial, temporal or other forms of dependence in the data)
5.  No multicollinearity

Additionally, OLS regression relies on continuous variables. In this assignment, we are dealing with binary variables as well as continuous variables, so regular OLS regression is inappropriate.

### Logistic Regression

#### Assumptions of Logistic Regression

Instead of OLS regression, we will use an approach known as logistic regression. The assumptions of logistic regression are similar to the assumptions in OLS regression, but not identical. In logistic regression:

1.  The dependent variable must be binary
2.  Observations must be independent
3.  There must not be multicollinearity

We will also need a larger sample size than in OLS because in logistic regression we rely on maximum likelihood estimation. Logistic regression also differs from OLS in that:

1.  There's no assumption of a linear relationship between the DV and each IV
2.  There's no assumption of hetereoscedasticity
3.  Residuals do not need to be normally distributed

#### Odds and Odds Ratios

In order to understand logistic regression, we must first review the concept of *odds*.

Odds are similar to probability. However, while probability may be calculated as $\frac{\text{\# desirable outcomes}}{\text{\# possible outcomes}}$, odds are calculated as $\frac{\text{\# desirable outcomes}}{\text{\# undesirable outcomes}}$.

![Technically, it's the *probability* of being murdered by a serial killer](https://www.smbc-comics.com/comics/1482164544-20161219.png){width="50%"}

The log odds that $Y = 1$ (or the *logit*) are simply $ln(Odds(Y = 1))$. In other words, if $Y$ is our dependent variable and $X$ is our independent variable, a 1-unit increase in $X$ means that the odds that $Y=1$ go up by $e^\text{ln(Odds(Y = 1))}$.

The *odds ratio* is the ratio of the odds of one outcome for our binary dependent variable versus the other possible outcome, or $\frac{\text{Odds(Y=1)}}{\text{Odds(Y=0)}}$. It can also be calculated by exponentiating the coefficient of the independent variable in question. The odds ratio indicates how much more likely we are to see $Y=1$ than $Y=0$, e.g., an odds ratio of 3 means that it is 3 times more likely that $Y=1$ than that $Y=0$.[^1] Rather than looking at the estimated $\beta$ coefficients, most statisticians prefer to look at odds ratios

[^1]: Sperandei S. Understanding logistic regression analysis. Biochem Med (Zagreb). 2014 Feb 15;24(1):12-8. doi: 10.11613/BM.2014.003. PMID: 24627710; PMCID: PMC3936971.

#### Equation for the Logistic Regression and Multivariate Logistic Regression

For logistic regression with a single predictor, the equation is $$ln(\frac{p}{1-p}) = \beta_0 + \beta_1X_1 + \epsilon$$

where $p = P(Y=1)$,

$\frac{p}{1-p}$ are the odds,

and $ln(\frac{p}{1-p})$ are the log odds, or logit.

Solving algebraically, we come to the *inverse logit* or *logistic* function for logistic regression with one variable: $$p = \frac{e^{\beta_0+\beta_1X_1}}{1+e^{\beta_0+\beta_1X_1}} = \frac{1}{1+e^{-\beta_0-\beta_1X_1}}$$

For multivariate logistic regression with $n$ predictors, that equation becomes $$p = \frac{e^{\beta_0+\beta_1X_1+\beta_2X_2+...+\beta_nX_n}}{1+e^{\beta_0+\beta_1X_1+\beta_2X_2+...+\beta_nX_n}}$$

### Interpretation and Hypotheses for Each Predictor

As explained above, we can say that $e^{\beta_1}$ is the extent to which the odds of $Y=1$ change as the predictor (i.e., Population) increases by 1 unit. So, when $e^{\beta_1} = 1.001$, 1.001 is the extent to which the odds that $Y=1$ go up as the independent variable $X$ increases by 1 unit. If we want to examine the extent to which the odds that $Y=1$ change as X increases by 100 units, we can simply raise $e$ to $100*\beta_1$, so $e^{100*\beta_1}$. If $\beta_1 < 0$, we would say that the odds that $Y=1$ *decrease* by a factor of $e^{\beta_1}$ as X *increases* by 1 unit. If $\beta_1 = 0$, then the predictor has *no effect* on the dependent variable.

For each predictor $X_i$:

-   $H_0:\beta_i = 0 \hspace{1mm} (OR_i = 1)$
-   $H_a:\beta_i \neq 0 \hspace{1mm} (OR_i \neq 1)$

Within the context of logistic regression, the z-score is equal to $\frac{\hat{\beta_i}}{\sigma\hat{\beta_i}}$, which is normally distributed. This is also sometimes known as the *Wald statistic*, and yields a p-value for each term in the regression using the standard normal (z) tables and in the R output.

### Assessing Quality of Model Fit

With logistic regression, it is possible to calculate an $R^2$ value and, generally speaking, the higher the $R^2$, the better. However, this value is rarely used, as it cannot be interpreted as the percent of variance that is explained by the model.

Instead, we can use the Akaike Information Criterion to compare models. The AIC estimates prediction error, and therefore the quality of our model, and includes a penality that "is an increasing function of the number of estimated parameters".[^2] When comparing multiple models, "the one with the minimum AIC value assures a good balance of goodness of fit and complexity".[^3]

[^2]: https://en.wikipedia.org/wiki/Akaike_information_criterion

[^3]: https://www.sciencedirect.com/topics/social-sciences/akaike-information-criterion

Finally, in assessing the quality of our model, we can consider *specificity*, *sensitivity*, and the *misclassification rate*.

Sensitivity (also called the true positive rate) measures the proportion of actual positives which are correctly identified as such (e.g., the percentage of sick people who are correctly identified as having the condition), and is complementary to the false negative rate. Specificity (also called the true negative rate) measures the proportion of negatives which are correctly identified as such (e.g., the percentage of healthy people who are correctly identified as not having the condition), and is complementary to the false positive rate. The misclassification rate is the proportion of the sum of false positives and false negatives out of the total number of observations in the sample.

![False positives are dangerous, folks](https://www.smbc-comics.com/comics/1468070206-20160709.png){width="50%"}

A simple way of assessing model quality is to determine a cut-off value for the fitted values $\hat{y_i}$, which is equal to P(Y = 1) at each observation $i$. When the model performs well, observations where $Y = 1$ will have higher values of $\hat{y_i}$, and observations where $Y = 0$ will have lower values of $\hat{y_i}$. We can set a cut-off value, ranging between 0 and 1, for what constitutes a "high" versus a "low" value based on the distribution of our data. In practice, most statisticians use multiple values, e.g., 0.1, 0.2, 0.3 ... 0.7.

Ideally, we want to choose a cut-off value that will optimize sensitivity and specificity. Typically, we want to see a high true positive rate and a low false positive rate, although, depending on the specific case, it may be more important to minimize the false positive rate than to maximize the true positive rate, or vice versa. To this end, we can plot the true positive rate (sensitivity) against the false positive rate (specificity), as in Figure 1[^4]. This is known an ROC curve, and it offers us a few different options for identifying out probability cut-off:

[^4]: Source: https://en.wikipedia.org/wiki/Receiver_operating_characteristic

1.  The *Youden Index* is a cut-off for which the sum of sensitivity plus specificity is maximized
2.  A cut-off for which the ROC curve has the minimum distance from the upper left corner of the graph, i.e., the point at which specificy = 1 and sensitivity = 1 (this can be implemented in R)

In this assignment, we will be selecting our cut-off by taking the second approach and minimizing the distance of the curve from the upper left corner of the graph.

![Figure 1: ROC Curve Example](https://raw.githubusercontent.com/nlebovits/musa-500-hmwk-3/main/Roc_curve.svg){width="50%"}

If we calculate the area under our ROC curve (the AUC), we can measure the prediction accuracy of the model. It is effectively the probability that the model correctly ranks two randomly selected observations where one has $Y = 1$ and the other has $Y = 0$, i.e., the probability that the $\hat{y}$ for the observation where one has $Y = 1$ is higher than the $\hat{y}$ where $Y = 0$. Higher AUCs mean that we can find a cut-off value for which both sensitivity and specificity are relatively high. The AUC varies from 0.5 to 1, and roughly correspond to the following accuracy levels:

-   .9 to 1.0 = excellent
-   .8 to .9 = good
-   .7 to .8 = fair (some statisticians say that any value \> .7 is good)
-   .6 to .7 = poor
-   .5 to .6 = fail

### Exploratory Analysis Prior to Regression

Prior to running our regression model, we will run cross tabulations between the dependent variable DRINKING_D and each of the *binary* predictors (FATAL_OR_M, OVERTURNED, CELL_PHONE, SPEEDING, AGGRESSIVE, DRIVER1617, DRIVER65PLUS). A cross tabulation will show us the proportional distribution of the variables. More importantly for our purposes, we can use a Chi-squared test to examine the association between the DV and each individual predictor variable. Here, the p-value is calculated based on the $\chi^2$ value and the degrees of freedom. If p \< 0.05, then the relationship between the dependent variable and the given predictor is significant. If we have a high $\chi^2$ and a p-value \< 0.05, we can therefore reject $H_0$, that the proportion of fatalities for crashes in which our predictor = 1 *is* the same as the proportion of fatalities for crashes in which our predictor = 0, in favor of $H_a$, that the proportion of fatalities for crashes in which our predictor = 1 is *not* the same as the proportion of fatalities for crashes in which our predictor = 0. This suggests a relationship between the variables.

Additionally, we can compare the means of the *continuous* predictors in our model (PCTBACHMOR and MEDHHINC) by looking at independent sample t-tests. Here, $H_0$ is that the average values for our continuous predictor variable are *the same* for crashes that involve drunk drivers and crashes that don't. $H_a$ is that the average values for our continuous predictor variable are *different* for crashes that involve drunk drivers and crashes that don't. As before, a high value of the t-statistic, and a p-value lower than 0.05 suggest that there's evidence to reject the null hypothesis in favor of the alternative.

## Results

### Findings from Exploratory Analysis

#### Distribution of Dependent Variable

To begin our data exploration, we'll consider the distribution of our dependent variable. As we can see, from our prop table, crashes in which the driver were drunk account for only 5.7% of all 43,364 crashes between 2008 and 2012.

```{r dependent variable}

drinking_d_table = table(crash_data$drinking_d)

DV_table <- left_join(as.data.frame(drinking_d_table), as.data.frame(prop.table(drinking_d_table)), by = "Var1") %>%
  mutate(Var1 = case_when(Var1 == 0 ~ "Non-drunk",
                          Var1 == 1 ~ "Drunk"),
         Freq.y = round(Freq.y, 3))

kable(DV_table, caption = "<center><span style='font-size:14px; color:black; font-family: Arial'>Table 2-1-1. Tabulation of the Dependent Variable</span></center>",
      col.names = c("Driver Status", "Count", "Proportion"), align = "c") %>% 
    kable_minimal(full_width = T, html_font = "Arial", font_size = 14) %>%
    row_spec(0:2, extra_css = "line-height: 30px")

```

#### Cross-Tabulations with Binary Predictors

Next, we'll look at the cross-tabulations of our dependent variable with each of the binary predictors to evaluate whether the Chi-squared test indicates a statistically significant relationship.

```{r cross tabulation}
#| tbl-cap: "Cross-Tabulations"
#| column: screen-inset-shaded
#| layout-nrow: 4

crosstable(crash_data, fatal_or_m, by = drinking_d, funs = "mean", test = TRUE) %>% 
  as_flextable()

crosstable(crash_data, overturned, by = drinking_d, funs = "mean", test = TRUE) %>% 
  as_flextable()

crosstable(crash_data, cell_phone, by = drinking_d, funs = "mean", test = TRUE) %>% 
  as_flextable()

crosstable(crash_data, speeding, by = drinking_d, funs = "mean", test = TRUE) %>% 
  as_flextable()

crosstable(crash_data, aggressive, by = drinking_d, funs = "mean", test = TRUE) %>% 
  as_flextable()

crosstable(crash_data, driver1617, by = drinking_d, funs = "mean", test = TRUE) %>% 
  as_flextable()

crosstable(crash_data, driver65plus, by = drinking_d, funs = "mean", test = TRUE) %>% 
  as_flextable()

#CrossTable(crash_data$fatal_or_m, crash_data$drinking_d, prop.r = FALSE, prop.t = FALSE, prop.chisq = FALSE, chisq = TRUE)

#CrossTable(crash_data$overturned, crash_data$drinking_d, prop.r = FALSE, prop.t = FALSE, prop.chisq = FALSE, chisq = TRUE)

#CrossTable(crash_data$cell_phone, crash_data$drinking_d, prop.r = FALSE, prop.t = FALSE, prop.chisq = FALSE, chisq = TRUE)

#CrossTable(crash_data$speeding, crash_data$drinking_d, prop.r = FALSE, prop.t = FALSE, prop.chisq = FALSE, chisq = TRUE)

#CrossTable(crash_data$aggressive, crash_data$drinking_d, prop.r = FALSE, prop.t = FALSE, prop.chisq = FALSE, chisq = TRUE)

#CrossTable(crash_data$driver1617, crash_data$drinking_d, prop.r = FALSE, prop.t = FALSE, prop.chisq = FALSE, chisq = TRUE)

#CrossTable(crash_data$driver65plus, crash_data$drinking_d, prop.r = FALSE, prop.t = FALSE, prop.chisq = FALSE, chisq = TRUE)
```

As we can see based on these cross-tabulations, the Chi-squared test shows a high $\chi^2$ and a p-value \< 0.05 for the dependent variable and each of the binary predictors except for CELL_PHONE. For all variables except for CELL_PHONE, then, we can reject $H_0$, which, as we stated before, states that there is that the probability of a crash involving a drunk driver is no different whether our predictor = 1 or = 0. In other words, we can reject the hypothesis that there is *no* relationship between the dependent variable and all of the predictors *other than CELL_PHONE*.

#### Comparing Means of Continuous Variables

    iii. Present the means of the continuous predictors for both values of the dependent variable (table on p. 5). 1. In the table, add an extra column which presents the results of the independent samples t-test (you may present the p-value only, or the t-statistic, the degrees of freedom and the p-value).

```{r means by group}
means = c(tapply(crash_data$pctbachmor, crash_data$drinking_d, mean), tapply(crash_data$medhhinc, crash_data$drinking_d, mean))
sds = c(tapply(crash_data$pctbachmor, crash_data$drinking_d, sd), tapply(crash_data$medhhinc, crash_data$drinking_d, sd))

contvars_df = as.data.frame(rbind(means, sds)) 

colnames(contvars_df) = c("PCTBACHMOR 0", "PCTBACHMOR 1", "MEDHHINC 0", "MEDHHINC 1")

contvars_df
```

#### T-Tests for Continuous Predictors

Discuss whether the t-test shows that there is a significant association between the dependent variable and each of the continuous predictors (i.e., can you reject the Null Hypothesis?).

```{r ttests}
#| column: screen-inset-shaded
#| layout-nrow: 1
t.test(crash_data$pctbachmor~crash_data$drinking_d)
t.test(crash_data$medhhinc~crash_data$drinking_d)
```

### Assumptions of Logistic Regression

-   Recalling from Method Section that there are three main assumptions for logistic regressions: 1) binary dependent variable, 2) independent observations, and 3) no severe multicollinearity. The dependent variable, DRINKING_D (whether drivers were drunk or not drunk) was binary, and the observations of each recorded car accident are independent of each other.

-   Multicollinearity usually refers to the situation when two or more predictors (explanatory variables) are strongly correlated with each other (r \> 0.9 or r \< -0.9), which signals a redundancy in including both predictors (in other words, adding the second one does not help improve the predictive power of the model). In the case of multicollinearity, we usually leave one predictor and take out the rest that are strongly correlated with that one predictor.

```{r multicollinearlity check}

#3-a multicollinearlity test

correlation <- crash_data[c(4:10, 12:13)]
cor(correlation, method = "pearson")

mc_test <- cor(correlation,use="pairwise.complete.obs")
corrplot(mc_test, method = "square",type = "lower",tl.cex = 0.5)

```

-   To test the third assumption, whether there is multicollinearity between all predictors (both binary and continuous), we used the Pearson correlations and results are shown below in the form of matrix. Note that since Pearson is usually run based on the assumption of continuous (not categorical) variables, there are potential limitations of using Pearson correlations to measure the association between binary predictors, since Pearson cannot determine non-linear relationships.

### Logistic Regression Results

-   

    i.  First, present the results of the logistic regression with all predictors (FATAL_OR_M, OVERTURNED, CELL_PHONE, SPEEDING, AGGRESSIVE, DRIVER1617, DRIVER65PLUS, PCTBACHMOR, and MEDHHINC).

```{r regression model}

#3-a 

logit <- glm(drinking_d ~ fatal_or_m + overturned + cell_phone + speeding + aggressive + driver1617 + driver65plus
             + pctbachmor + medhhinc, data = crash_data, family = "binomial")
summary(logit)

exp(cbind(OR = coef(logit), confint(logit)))

logitoutput <- summary(logit)
logitcoeffs <- logitoutput$coefficients
logitcoeffs

or_ci <- exp(cbind(OR = coef(logit), confint(logit)))

finallogitoutput <- cbind(logitcoeffs, or_ci)
finallogitoutput

```

-   

    1.  Interpret the results. Be sure to comment on whether each predictor is significant, and interpret the odds ratio for each predictor.

<!--writings-->

-   

    2.  State whether there is evidence of multicollinearity, and remind the reader how you're defining multicollinearity.

<!--writings-->

-   

    ii. Using the table on page 7, present the specificity, sensitivity and the misclassification rates for the different probability cut-offs. Indicate the cut-offs which yield the lowest/highest misclassification rates.

```{r sensitivity, specificity}
#| tbl-cap: "Cross-Tabulations, Fit Binary"
#| column: screen-inset-shaded
#| layout-nrow: 4

fit <- logit$fitted.values

fit.binary = (fit>=0.02)
CrossTable(fit.binary, crash_data$drinking_d, prop.r = FALSE, prop.t = FALSE, prop.chisq = FALSE)

fit.binary = (fit>=0.03)
CrossTable(fit.binary, crash_data$drinking_d, prop.r = FALSE, prop.t = FALSE, prop.chisq = FALSE)

fit.binary = (fit>=0.05)
CrossTable(fit.binary, crash_data$drinking_d, prop.r = FALSE, prop.t = FALSE, prop.chisq = FALSE)

fit.binary = (fit>=0.07)
CrossTable(fit.binary, crash_data$drinking_d, prop.r = FALSE, prop.t = FALSE, prop.chisq = FALSE)

fit.binary = (fit>=0.08)
CrossTable(fit.binary, crash_data$drinking_d, prop.r = FALSE, prop.t = FALSE, prop.chisq = FALSE)

fit.binary = (fit>=0.09)
CrossTable(fit.binary, crash_data$drinking_d, prop.r = FALSE, prop.t = FALSE, prop.chisq = FALSE)

fit.binary = (fit>=0.1)
CrossTable(fit.binary, crash_data$drinking_d, prop.r = FALSE, prop.t = FALSE, prop.chisq = FALSE)

fit.binary = (fit>=0.15)
CrossTable(fit.binary, crash_data$drinking_d, prop.r = FALSE, prop.t = FALSE, prop.chisq = FALSE)

fit.binary = (fit>=0.2)
CrossTable(fit.binary, crash_data$drinking_d, prop.r = FALSE, prop.t = FALSE, prop.chisq = FALSE)

fit.binary = (fit>=0.5)
CrossTable(fit.binary, crash_data$drinking_d, prop.r = FALSE, prop.t = FALSE, prop.chisq = FALSE)

```

-   

    iii. Present the ROC curve and comment on the optimal cut-off rate that was selected by minimizing the distance from the upper left corner of the ROC curve.

1.  Compare this cut-off rate with the optimal rate in 3.c.ii above

<!-- -->

a.  Keep in mind that in 3.c.ii, we're looking at minimum mis-classification rates, and here, we're looking at simultaneously minimizing both sensitivity and specificity.

```{r ROC}

#3-a 

a <- cbind(crash_data$drinking_d, fit)
colnames(a) <- c("labels", "predictions")
head(a)
roc <- as.data.frame(a)
pred <- ROCR::prediction(roc$predictions, roc$labels)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
plot(roc.perf)
abline(a = 0, b = 1)


```

```{r optimal cutoff}

#3-a 

opt.cut = function(perf, pred){
  cut.ind = mapply(FUN=function(x, y, p){
    d = (x - 0)^2 + (y-1)^2
    ind = which(d == min(d))
    c(sensitivity = y[[ind]], specificity = 1-x[[ind]], 
      cutoff = p[[ind]])
  }, perf@x.values, perf@y.values, pred@cutoffs)
}

print(opt.cut(roc.perf, pred))

```

-   

    iv. Also present and comment on the area under the ROC curve. What does it tell us about our model?

```{r calculate AUC}

#3-a 

auc.perf = performance(pred, measure ="auc")
auc.perf@y.values

```

-   

    v.  Finally, present the results of the logistic regression with the binary predictors only (i.e., without PCTBACHMOR and MEDHHINC). 1. Compare the results of this regression with the results of the first regression: are there any predictors which are significant in the new model which weren't significant in the original one, or vice versa?

```{r regression model2}

#3-a 

logit2 <- glm(drinking_d ~ fatal_or_m + overturned + cell_phone + speeding + aggressive + driver1617 + driver65plus
            , data = crash_data, family = "binomial")

summary(logit2)

exp(cbind(OR = coef(logit2), confint(logit2)))

logitoutput2 <- summary(logit2)
logitcoeffs2 <- logitoutput2$coefficients
logitcoeffs2

or_ci2 <- exp(cbind(OR = coef(logit2), confint(logit2)))

finallogitoutput2 <- cbind(logitcoeffs2, or_ci2)
finallogitoutput2

```

2.  Be sure to also present the Akaike Information Criterion (AIC) for both models and indicate which model is better.

```{r AIC}

AIC(logit, logit2)

```

Similar to OLS, we can do cross-validation to determine a quality of the model. See following links for more information: K-Fold cross-validation in R: https://www.r-bloggers.com/evaluating-logistic-regression-models/ More on cross-validation in R: https://www.r-bloggers.com/predicting-creditability-using-logistic-regression-in-r-cross-validating-the-classifier-part-2-2/

## Discussion

-   

    a)  In a couple sentences, recap what you did in the paper, and your findings.

-   

    i.  Which variables are strong predictors of crashes that involve drunk driving? Which variables aren't associated with the dependent variable?

-   

    ii. Are the results surprising? Discuss, and mention whether the variables you expect to be significant actually are significant, and if so, whether the relationships with the dependent variable are in the direction you would expect.

-   

    iii. Is logistic regression appropriate here? In other words, would the modeling rare events methods proposed by Paul Allison be more appropriate here?

-   

    1.  Hint: look at the \# and % of cases with values of '1' for the dependent variable.

-   

    b)  What are some limitations of the analysis? Discuss.

### Recap

### Limitations
